ubuntu@ip-172-31-0-203:~/galaxy_classification/code$ python keras_NN.py 
Using Theano backend.
Using gpu device 0: GRID K520 (CNMeM is disabled, CuDNN not available)
/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.
  warnings.warn("downsample module has been moved to the pool module.")
Loading Data
test/train split
scaling data
building model
compiling model
fitting model
Epoch 1/100
36s - loss: 1.0198 - acc: 0.4115 - val_loss: 0.9908 - val_acc: 0.4188
Epoch 2/100
36s - loss: 0.9842 - acc: 0.4140 - val_loss: 0.9624 - val_acc: 0.4678
Epoch 3/100
36s - loss: 0.9335 - acc: 0.5111 - val_loss: 0.8460 - val_acc: 0.6032
Epoch 4/100
36s - loss: 0.8452 - acc: 0.5829 - val_loss: 0.7717 - val_acc: 0.6270
Epoch 5/100
36s - loss: 0.8042 - acc: 0.6083 - val_loss: 0.7492 - val_acc: 0.6426
Epoch 6/100
36s - loss: 0.7840 - acc: 0.6216 - val_loss: 0.7173 - val_acc: 0.6614
Epoch 7/100
36s - loss: 0.7593 - acc: 0.6379 - val_loss: 0.7160 - val_acc: 0.6618
Epoch 8/100
36s - loss: 0.7445 - acc: 0.6476 - val_loss: 0.7097 - val_acc: 0.6624
Epoch 9/100
36s - loss: 0.7327 - acc: 0.6543 - val_loss: 0.7105 - val_acc: 0.6558
Epoch 10/100
36s - loss: 0.7086 - acc: 0.6656 - val_loss: 0.6829 - val_acc: 0.6754
Epoch 11/100
36s - loss: 0.6836 - acc: 0.6791 - val_loss: 0.6438 - val_acc: 0.6956
Epoch 12/100
36s - loss: 0.6784 - acc: 0.6829 - val_loss: 0.6149 - val_acc: 0.7120
Epoch 13/100
36s - loss: 0.6529 - acc: 0.6946 - val_loss: 0.6022 - val_acc: 0.7158
Epoch 14/100
36s - loss: 0.6466 - acc: 0.6990 - val_loss: 0.5943 - val_acc: 0.7166
Epoch 15/100
36s - loss: 0.6308 - acc: 0.7062 - val_loss: 0.5917 - val_acc: 0.7222
Epoch 16/100
36s - loss: 0.6224 - acc: 0.7089 - val_loss: 0.5928 - val_acc: 0.7180
Epoch 17/100
36s - loss: 0.6163 - acc: 0.7141 - val_loss: 0.6512 - val_acc: 0.6948
Epoch 18/100
36s - loss: 0.6083 - acc: 0.7183 - val_loss: 0.5746 - val_acc: 0.7340
Epoch 19/100
36s - loss: 0.6094 - acc: 0.7157 - val_loss: 0.5910 - val_acc: 0.7262
Epoch 20/100
35s - loss: 0.6118 - acc: 0.7189 - val_loss: 0.5537 - val_acc: 0.7408
Epoch 21/100
35s - loss: 0.5926 - acc: 0.7292 - val_loss: 0.5846 - val_acc: 0.7230
Epoch 22/100
35s - loss: 0.5917 - acc: 0.7262 - val_loss: 0.5604 - val_acc: 0.7392
Epoch 23/100
35s - loss: 0.5866 - acc: 0.7246 - val_loss: 0.5515 - val_acc: 0.7434
Epoch 24/100
35s - loss: 0.5908 - acc: 0.7253 - val_loss: 0.5514 - val_acc: 0.7426
Epoch 25/100
35s - loss: 0.5886 - acc: 0.7262 - val_loss: 0.5475 - val_acc: 0.7508
Epoch 26/100
35s - loss: 0.5902 - acc: 0.7290 - val_loss: 0.5625 - val_acc: 0.7394
Epoch 27/100
35s - loss: 0.5832 - acc: 0.7283 - val_loss: 0.5354 - val_acc: 0.7536
Epoch 28/100
35s - loss: 0.5783 - acc: 0.7324 - val_loss: 0.5409 - val_acc: 0.7514
Epoch 29/100
35s - loss: 0.5784 - acc: 0.7364 - val_loss: 0.5346 - val_acc: 0.7504
Epoch 30/100
35s - loss: 0.5799 - acc: 0.7334 - val_loss: 0.5419 - val_acc: 0.7494
Epoch 31/100
35s - loss: 0.5753 - acc: 0.7314 - val_loss: 0.5577 - val_acc: 0.7374
Epoch 32/100
35s - loss: 0.5710 - acc: 0.7361 - val_loss: 0.5540 - val_acc: 0.7452
Epoch 33/100
35s - loss: 0.5768 - acc: 0.7331 - val_loss: 0.5339 - val_acc: 0.7514
Epoch 34/100
35s - loss: 0.5726 - acc: 0.7357 - val_loss: 0.5400 - val_acc: 0.7478
Epoch 35/100
35s - loss: 0.5725 - acc: 0.7384 - val_loss: 0.5322 - val_acc: 0.7518
Epoch 36/100
35s - loss: 0.5645 - acc: 0.7393 - val_loss: 0.5407 - val_acc: 0.7510
